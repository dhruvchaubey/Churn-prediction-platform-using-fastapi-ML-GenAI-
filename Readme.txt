FastAPI Churn Prediction Application
This project provides a FastAPI-based application to predict customer churn using traditional machine learning models and to generate natural language explanations for each prediction using the Gemini API.

System Overview
The application has the following endpoints:

/upload: Upload a CSV file and specify the target column.

/data_cleaning: Configure and apply preprocessing steps (imputation, scaling, encoding).

/train_model: Select and train one or more machine learning models.

/predict: Use a trained model to predict on new data and generate an explanation for each prediction.

Setup Instructions
Save the files:

Save the code from the first block as main.py.

Save the content of the requirements.txt block into a file named requirements.txt.

Place the churn (1).csv file in the same directory.

Note: Remember to add your Gemini API key to main.py for the prediction endpoint to work locally.

Install Dependencies:

pip install -r requirements.txt

Run the Application:

uvicorn main:app --reload

The application will be accessible at http://127.0.0.1:8000. You can interact with the API endpoints via the interactive documentation at http://127.0.0.1:8000/docs.

GenAI Prompt Structure
The natural language explanations are generated by the Gemini API. The prompt is constructed dynamically within the /predict endpoint based on the customer's data and the model's prediction. The structure is as follows:

"Analyze the following customer data and provide a concise, natural language explanation "
"for why the customer is likely to {churn or not churn}. Focus on key features like CreditScore, "
"Geography, Age, Tenure, Balance, and EstimatedSalary. "
"The prediction is based on the data: {data_str}"

This prompt guides the AI to focus on specific, relevant features from the customer's profile to create a human-readable summary of the prediction.

Sample Usage with cURL
You can use the following curl commands in a separate terminal to interact with the API endpoints.

Step 1: Upload the Dataset
This command uploads the churn (1).csv file and sets Exited as the target column.

curl -X 'POST' \
  'http://127.0.0.1:8000/upload' \
  -H 'accept: application/json' \
  -H 'Content-Type: multipart/form-data' \
  -F 'file=@churn (1).csv;type=text/csv' \
  -F 'target_column=Exited'

Step 2: Configure and Clean the Data
This command applies the default data cleaning and preprocessing steps.

curl -X 'POST' \
  'http://127.0.0.1:8000/data_cleaning' \
  -H 'accept: application/json' \
  -H 'Content-Type: application/json' \
  -d '{
  "missing_value_strategy": "impute_mean",
  "categorical_encoding": "one_hot",
  "scaling_method": "standard"
}'

Step 3: Train the Models
This command trains a Logistic Regression and a Random Forest Classifier model.

curl -X 'POST' \
  'http://127.0.0.1:8000/train_model' \
  -H 'accept: application/json' \
  -H 'Content-Type: application/json' \
  -d '{
  "model_names": [
    "LogisticRegression",
    "RandomForestClassifier"
  ]
}'

Step 4: Get Predictions and Explanations
This command uses the trained RandomForestClassifier to generate predictions on a test CSV file and saves the JSON output to predictions.json.

curl -X 'POST' \
  'http://127.0.0.1:8000/predict' \
  -H 'accept: application/json' \
  -H 'Content-Type: multipart/form-data' \
  -F 'file=@test_churn.csv;type=text/csv' \
  -F 'model_name=RandomForestClassifier' > predictions.json
