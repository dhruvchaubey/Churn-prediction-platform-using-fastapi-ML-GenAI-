# -*- coding: utf-8 -*-
"""app

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10M0S8X_lykVoWwThpoppxZSEbiWHmZtU
"""

# main.py
import os
import pandas as pd
import numpy as np
import io
import joblib
import httpx
from fastapi import FastAPI, UploadFile, File, Form, HTTPException
from pydantic import BaseModel
from typing import List, Dict, Optional, Any

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

# --- FastAPI App & State Management ---
app = FastAPI(title="Churn Prediction & Explanation API")

# In-memory storage for the application state (models, data, transformers)
# In a production environment, this would be replaced by a more robust solution
# like a database or file storage for different users.
app_state = {
    "df": None,
    "target_column": None,
    "feature_columns": None,
    "cleaned_df": None,
    "data_pipeline": None,
    "trained_models": {},
}

# --- Pydantic Models for API Validation ---
class DataCleaningRequest(BaseModel):
    """Schema for the data cleaning request body."""
    missing_value_strategy: str = "impute_mean"  # 'drop' or 'impute_mean'
    categorical_encoding: str = "one_hot"       # 'one_hot' or 'label'
    scaling_method: str = "standard"            # 'standard' or 'min_max'

class ModelTrainingRequest(BaseModel):
    """Schema for the model training request body."""
    model_names: List[str]  # e.g., ['LogisticRegression', 'RandomForestClassifier']

class PredictionRequest(BaseModel):
    """Schema for the prediction request body."""
    model_name: str
    data: List[Dict[str, Any]] # Test data as a list of dictionaries

# --- Helper Functions ---
def get_model(model_name):
    """Returns an instance of a scikit-learn model based on the name."""
    if model_name == 'LogisticRegression':
        return LogisticRegression(random_state=42)
    elif model_name == 'RandomForestClassifier':
        return RandomForestClassifier(random_state=42)
    # Add other models here
    else:
        raise ValueError(f"Unknown model name: {model_name}")

def get_preprocessor_pipeline(missing_value_strategy, categorical_encoding, scaling_method, numeric_cols, categorical_cols):
    """Builds and returns a scikit-learn preprocessing pipeline."""
    # Handle missing values
    numeric_imputer = SimpleImputer(strategy='mean')
    categorical_imputer = SimpleImputer(strategy='most_frequent')

    # Scaling
    if scaling_method == 'standard':
        scaler = StandardScaler()
    elif scaling_method == 'min_max':
        scaler = MinMaxScaler()
    else:
        raise ValueError("Invalid scaling method. Choose 'standard' or 'min_max'.")

    # Encoding
    if categorical_encoding == 'one_hot':
        encoder = OneHotEncoder(handle_unknown='ignore')
    elif categorical_encoding == 'label':
        # LabelEncoder can't be used in a ColumnTransformer directly on multiple columns.
        # This is a simplification; a better approach would be to use a loop or
        # another custom transformer, but for this demo, we'll stick to OneHot.
        # We will apply Label Encoding separately if needed.
        raise ValueError("Label encoding is not supported in this pipeline setup. Use 'one_hot'.")
    else:
        raise ValueError("Invalid categorical encoding. Choose 'one_hot' or 'label'.")

    numeric_transformer = Pipeline(steps=[
        ('imputer', numeric_imputer),
        ('scaler', scaler)
    ])

    categorical_transformer = Pipeline(steps=[
        ('imputer', categorical_imputer),
        ('encoder', encoder)
    ])

    preprocessor = ColumnTransformer(
        transformers=[
            ('num', numeric_transformer, numeric_cols),
            ('cat', categorical_transformer, categorical_cols)
        ],
        remainder='passthrough'
    )

    return preprocessor

# --- Gemini API interaction ---
async def get_genai_explanation(row_data: Dict[str, Any], prediction: int):
    """
    Calls the Gemini API to get a natural language explanation for a prediction.
    """
    prediction_text = "churn" if prediction == 1 else "not churn"

    # Convert row_data dictionary to a string for the prompt
    data_str = ", ".join([f"{k}: {v}" for k, v in row_data.items()])

    prompt = (
        f"Analyze the following customer data and provide a concise, natural language explanation "
        f"for why the customer is likely to {prediction_text}. Focus on key features like CreditScore, "
        f"Geography, Age, Tenure, Balance, and EstimatedSalary. "
        f"The prediction is based on the data: {data_str}"
    )

    api_key = "AIzaSyB7Q0XFwHQpdYVrQ4G6wKb5w3y0WLhqAv4" # API key is automatically provided by the platform
    api_url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-05-20:generateContent?key={api_key}"

    headers = {
        'Content-Type': 'application/json'
    }

    payload = {
        "contents": [
            {"parts": [{"text": prompt}]}
        ]
    }

    try:
        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.post(api_url, headers=headers, json=payload)
            response.raise_for_status()  # Raise an exception for bad status codes
            result = response.json()
            explanation = result['candidates'][0]['content']['parts'][0]['text']
            return explanation
    except httpx.RequestError as exc:
        print(f"An error occurred while requesting Gemini API: {exc}")
        return "Explanation service is currently unavailable."
    except KeyError:
        print("Unexpected response structure from Gemini API.")
        return "Could not generate an explanation due to an unexpected API response."
    except Exception as e:
        print(f"An unexpected error occurred: {e}")
        return "An unexpected error occurred while generating the explanation."

# --- API Endpoints ---
@app.post("/upload")
async def upload_dataset(file: UploadFile = File(...), target_column: str = Form(...)):
    """
    Uploads a CSV file and specifies the target column for prediction.
    Returns dataset metadata including a preview.
    """
    try:
        contents = await file.read()
        df = pd.read_csv(io.StringIO(contents.decode('utf-8')))

        if target_column not in df.columns:
            raise HTTPException(status_code=400, detail=f"Target column '{target_column}' not found in the dataset.")

        # Store the dataframe and target column in app state
        app_state["df"] = df
        app_state["target_column"] = target_column
        app_state["feature_columns"] = [col for col in df.columns if col != target_column]

        # Prepare metadata
        column_types = {col: str(df[col].dtype) for col in df.columns}
        missing_values = df.isnull().sum().to_dict()
        preview = df.head().to_dict('records')

        return {
            "message": "Dataset uploaded successfully.",
            "filename": file.filename,
            "target_column": target_column,
            "metadata": {
                "columns": list(df.columns),
                "column_types": column_types,
                "missing_values": missing_values,
                "sample_preview": preview,
            }
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"An error occurred during file upload: {e}")

@app.post("/data_cleaning")
async def clean_data(cleaning_request: DataCleaningRequest):
    """
    Configures and applies data preprocessing steps.
    Returns a preview of the cleaned dataset.
    """
    df = app_state.get("df")
    target_column = app_state.get("target_column")
    feature_columns = app_state.get("feature_columns")

    if df is None or target_column is None:
        raise HTTPException(status_code=400, detail="No dataset found. Please upload a dataset first.")

    X = df[feature_columns]
    y = df[target_column]

    # Identify numeric and categorical columns for the pipeline
    numeric_cols = X.select_dtypes(include=np.number).columns.tolist()
    categorical_cols = X.select_dtypes(include=['object']).columns.tolist()

    try:
        # Build the preprocessing pipeline
        data_pipeline = get_preprocessor_pipeline(
            missing_value_strategy=cleaning_request.missing_value_strategy,
            categorical_encoding=cleaning_request.categorical_encoding,
            scaling_method=cleaning_request.scaling_method,
            numeric_cols=numeric_cols,
            categorical_cols=categorical_cols
        )

        # Fit and transform the data
        X_cleaned = data_pipeline.fit_transform(X)

        # We'll save the pipeline and the target column separately for prediction
        app_state["data_pipeline"] = data_pipeline

        # Store the cleaned DataFrame. For simplicity, we'll store X and y separately.
        # In a real app, you would handle this more robustly.
        app_state["cleaned_df_X"] = X_cleaned
        app_state["cleaned_df_y"] = y

        # Return a preview of the cleaned data
        # Note: The cleaned data is a numpy array. We'll show a sample
        # of the original data as a proxy for the preview.
        preview = df.head().to_dict('records')

        return {
            "message": "Data cleaning applied successfully.",
            "cleaning_config": cleaning_request.dict(),
            "preview_of_original_data": preview # Showing original preview as cleaned data is a numpy array
        }
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"An error occurred during data cleaning: {e}")

@app.post("/train_model")
async def train_model(training_request: ModelTrainingRequest):
    """
    Trains one or more machine learning models on the cleaned data.
    Returns evaluation metrics for each model.
    """
    X_cleaned = app_state.get("cleaned_df_X")
    y = app_state.get("cleaned_df_y")

    if X_cleaned is None or y is None:
        raise HTTPException(status_code=400, detail="Data has not been cleaned. Please run the data_cleaning step first.")

    # Split the data into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X_cleaned, y, test_size=0.2, random_state=42)

    results = {}

    for model_name in training_request.model_names:
        try:
            model = get_model(model_name)
            model.fit(X_train, y_train)

            y_pred = model.predict(X_test)

            # Calculate evaluation metrics
            accuracy = accuracy_score(y_test, y_pred)
            precision = precision_score(y_test, y_pred)
            recall = recall_score(y_test, y_pred)
            f1 = f1_score(y_test, y_pred)
            cm = confusion_matrix(y_test, y_pred).tolist()

            results[model_name] = {
                "accuracy": accuracy,
                "precision": precision,
                "recall": recall,
                "f1_score": f1,
                "confusion_matrix": cm
            }

            # Save the trained model
            app_state["trained_models"][model_name] = model

        except ValueError as e:
            raise HTTPException(status_code=400, detail=str(e))
        except Exception as e:
            raise HTTPException(status_code=500, detail=f"An error occurred while training model '{model_name}': {e}")

    return {
        "message": "Models trained successfully.",
        "training_results": results
    }

@app.post("/predict")
async def predict_churn(
    file: UploadFile = File(...),
    model_name: str = Form(...)
):
    """
    Generates predictions for a new CSV file and provides a GenAI summary.
    """
    trained_model = app_state["trained_models"].get(model_name)
    data_pipeline = app_state.get("data_pipeline")
    target_column = app_state.get("target_column")
    feature_columns = app_state.get("feature_columns")

    if trained_model is None:
        raise HTTPException(status_code=400, detail=f"Model '{model_name}' not found. Please train the model first.")
    if data_pipeline is None:
        raise HTTPException(status_code=400, detail="Preprocessing pipeline not found. Please run data cleaning first.")

    try:
        contents = await file.read()
        test_df = pd.read_csv(io.StringIO(contents.decode('utf-8')))

        # Ensure the test data has the same feature columns
        if not all(col in test_df.columns for col in feature_columns):
            missing_cols = [col for col in feature_columns if col not in test_df.columns]
            raise HTTPException(status_code=400, detail=f"Test data is missing required feature columns: {', '.join(missing_cols)}")

        test_df = test_df[feature_columns]

        # Apply the same preprocessing pipeline to the new data
        X_test_cleaned = data_pipeline.transform(test_df)

        # Get predictions
        predictions = trained_model.predict(X_test_cleaned)

        # Generate explanations for each prediction
        results = []
        for i, row in test_df.iterrows():
            prediction = predictions[i]
            explanation = await get_genai_explanation(row.to_dict(), prediction)

            results.append({
                "original_data": row.to_dict(),
                "prediction": int(prediction),
                "summary": explanation
            })

        return {
            "message": "Predictions generated successfully.",
            "results": results
        }
    except HTTPException as e:
        raise e
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"An error occurred during prediction: {e}")

# This part is for saving/loading models, but we are using in-memory state for this demo.
# In a production app, you would use this pattern.
def save_model(model, filename):
    joblib.dump(model, filename)

def load_model(filename):
    return joblib.load(filename)